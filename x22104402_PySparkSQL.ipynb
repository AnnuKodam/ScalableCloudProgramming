{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897a2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 10:40:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, ShortType, ByteType, DateType, TimestampType, LongType\n",
    "session = SparkSession.builder.appName(\"logs\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680739a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_schema = StructType([\n",
    "    StructField(\"alert\", StringType()),\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"date\", DateType()),\n",
    "    StructField(\"node\", StringType()),\n",
    "    StructField(\"timestamp2\", TimestampType()),\n",
    "    StructField(\"node2\", StringType()),\n",
    "    StructField(\"ras\", StringType()),\n",
    "    StructField(\"kernel\", StringType()),\n",
    "    StructField(\"info\", StringType()),\n",
    "    StructField(\"message\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b26df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = session.read.csv(\n",
    "    \"BGLnew.log\",\n",
    "    schema=logs_schema,\n",
    "    header=True,\n",
    "    dateFormat=\"yyyy.MM.dd\",\n",
    "    timestampFormat = \"yyyy-MM-dd-HH.mm.ss.SSSSSS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d22ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- alert: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- node: string (nullable = true)\n",
      " |-- timestamp2: timestamp (nullable = true)\n",
      " |-- node2: string (nullable = true)\n",
      " |-- ras: string (nullable = true)\n",
      " |-- kernel: string (nullable = true)\n",
      " |-- info: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15876c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "23/08/11 10:41:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: -, 1117838570, 2005.06.03, R02-M1-N0-C:J12-U11, 2005-06-03-15.42.50.363779, R02-M1-N0-C:J12-U11, RAS, KERNEL, INFO, instruction cache parity error corrected\n",
      " Schema: alert, timestamp, date, node, timestamp2, node2, ras, kernel, info, message\n",
      "Expected: alert but found: -\n",
      "CSV file: file:///home/anuhya/ScalableProject/BGLnew.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+-------------------+--------------------+-------------------+---+------+----+--------------------+\n",
      "|alert| timestamp|      date|               node|          timestamp2|              node2|ras|kernel|info|             message|\n",
      "+-----+----------+----------+-------------------+--------------------+-------------------+---+------+----+--------------------+\n",
      "|    -|1117838570|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838570|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838570|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838570|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838571|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838571|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838571|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838571|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838571|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838571|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838572|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838572|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838572|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838572|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838572|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838572|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838572|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838573|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838573|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "|    -|1117838573|2005-06-03|R02-M1-N0-C:J12-U11|2005-06-03 15:42:...|R02-M1-N0-C:J12-U11|RAS|KERNEL|INFO|instruction cache...|\n",
      "+-----+----------+----------+-------------------+--------------------+-------------------+---+------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:===========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "logs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3404450",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQuery1 = \"\"\"\n",
    "SELECT \n",
    "   COUNT(*) AS no_of_fatallogs\n",
    "FROM \n",
    "    logs\n",
    "WHERE \n",
    "    MONTH(date) = 12 -- December\n",
    "    AND message LIKE '%invalid or missing program image%'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3efb5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 10:43:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2005.06.03, instruction cache parity error corrected\n",
      " Schema: date, message\n",
      "Expected: date but found: 2005.06.03\n",
      "CSV file: file:///home/anuhya/ScalableProject/BGLnew.log\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|no_of_fatallogs|\n",
      "+---------------+\n",
      "|          18584|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(SQLQuery1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d08df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.createOrReplaceTempView(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295ab72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQuery5 = \"\"\"SELECT \n",
    "DATE_TRUNC('month', date) AS month,\n",
    "AVG(error_duration_seconds) AS avg_error_duration_per_month\n",
    "FROM \n",
    "  (\n",
    "    SELECT \n",
    "      date, \n",
    "      COUNT(*) AS error_duration_seconds\n",
    "    FROM logs\n",
    "    WHERE message LIKE '%EDRAM%' \n",
    "    GROUP BY date\n",
    "  ) \n",
    "GROUP BY\n",
    "  month\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6401307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 10:44:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2005.06.03, instruction cache parity error corrected\n",
      " Schema: date, message\n",
      "Expected: date but found: 2005.06.03\n",
      "CSV file: file:///home/anuhya/ScalableProject/BGLnew.log\n",
      "[Stage 4:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------------+\n",
      "|              month|avg_error_duration_per_month|\n",
      "+-------------------+----------------------------+\n",
      "|2005-12-01 00:00:00|          42.166666666666664|\n",
      "|2005-11-01 00:00:00|          28.133333333333333|\n",
      "|2006-01-01 00:00:00|                        25.0|\n",
      "|2005-10-01 00:00:00|          28.258064516129032|\n",
      "|2005-08-01 00:00:00|          23.285714285714285|\n",
      "|2005-09-01 00:00:00|          22.620689655172413|\n",
      "|2005-06-01 00:00:00|          49.607142857142854|\n",
      "|2005-07-01 00:00:00|          17.741935483870968|\n",
      "+-------------------+----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "session.sql(SQLQuery5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69e6585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQuery9 = \"\"\"\n",
    "    SELECT date, COUNT(*) as frequency\n",
    "    FROM logs\n",
    "    GROUP BY date\n",
    "    ORDER BY frequency DESC\n",
    "    LIMIT 5;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f57e5347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 10:46:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2005.06.03\n",
      " Schema: date\n",
      "Expected: date but found: 2005.06.03\n",
      "CSV file: file:///home/anuhya/ScalableProject/BGLnew.log\n",
      "[Stage 10:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|frequency|\n",
      "+----------+---------+\n",
      "|2005-07-09|   381827|\n",
      "|2005-06-14|   381561|\n",
      "|2005-12-01|   271341|\n",
      "|2005-11-03|   200937|\n",
      "|2005-07-23|   200654|\n",
      "+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "session.sql(SQLQuery9).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e3e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQuery15 = \"\"\"\n",
    "SELECT node, COUNT(*) as appbusy_events_count, COUNT(*) OVER() as total_entries\n",
    "FROM logs\n",
    "WHERE alert like '%KERNRTSP%'\n",
    "GROUP BY node\n",
    "ORDER BY appbusy_events_count DESC\n",
    "LIMIT 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c8e3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 10:46:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/11 10:46:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/11 10:46:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/11 10:46:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: -, R02-M1-N0-C:J12-U11\n",
      " Schema: alert, node\n",
      "Expected: alert but found: -\n",
      "CSV file: file:///home/anuhya/ScalableProject/BGLnew.log\n",
      "23/08/11 10:46:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/11 10:46:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/11 10:46:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/08/11 10:46:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|               node|appbusy_events_count|total_entries|\n",
      "+-------------------+--------------------+-------------+\n",
      "|R63-M0-NE-C:J12-U01|                  22|         3394|\n",
      "+-------------------+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "session.sql(SQLQuery15).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "719b2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQuery18 = \"\"\"\n",
    "SELECT \n",
    "    MIN(date) AS earliest_error_date\n",
    "FROM \n",
    "    logs\n",
    "WHERE \n",
    "    message LIKE '%Power Good signal deactivated%'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b36e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 10:47:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2005.06.03, instruction cache parity error corrected\n",
      " Schema: date, message\n",
      "Expected: date but found: 2005.06.03\n",
      "CSV file: file:///home/anuhya/ScalableProject/BGLnew.log\n",
      "[Stage 19:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|earliest_error_date|\n",
      "+-------------------+\n",
      "|         2005-11-17|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "session.sql(SQLQuery18).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee3013c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQuery2 = \"\"\"\n",
    "SELECT \n",
    "   COUNT(*) AS no_of_fatallogs\n",
    "FROM \n",
    "    logs\n",
    "WHERE \n",
    "    MONTH(date) = 9 -- September\n",
    "    AND message LIKE '%major internal error%'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "913cdb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 10:48:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2005.06.03, instruction cache parity error corrected\n",
      " Schema: date, message\n",
      "Expected: date but found: 2005.06.03\n",
      "CSV file: file:///home/anuhya/ScalableProject/BGLnew.log\n",
      "[Stage 22:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|no_of_fatallogs|\n",
      "+---------------+\n",
      "|             10|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "session.sql(SQLQuery2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcd657ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQuery11 = \"\"\"\n",
    "    SELECT node, COUNT(*) as frequency\n",
    "    FROM logs\n",
    "    GROUP BY node\n",
    "    ORDER BY frequency DESC\n",
    "    LIMIT 5;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfc9713d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 10:49:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: R02-M1-N0-C:J12-U11\n",
      " Schema: node\n",
      "Expected: node but found: R02-M1-N0-C:J12-U11\n",
      "CSV file: file:///home/anuhya/ScalableProject/BGLnew.log\n",
      "[Stage 25:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|               node|frequency|\n",
      "+-------------------+---------+\n",
      "|R30-M0-N9-C:J16-U01|   152329|\n",
      "|               NULL|    89296|\n",
      "|R02-M1-N0-C:J12-U11|    64650|\n",
      "|R37-M1-NC-C:J02-U11|    35288|\n",
      "|   UNKNOWN_LOCATION|    27039|\n",
      "+-------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 10:50:33 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-7a454bd9-abe0-47ee-b10d-4a1cae37ba16/userFiles-087dcaf7-eef5-4b9e-8072-abe4c30f8340. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-7a454bd9-abe0-47ee-b10d-4a1cae37ba16/userFiles-087dcaf7-eef5-4b9e-8072-abe4c30f8340\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/08/11 10:50:33 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: /tmp/spark-7a454bd9-abe0-47ee-b10d-4a1cae37ba16/userFiles-087dcaf7-eef5-4b9e-8072-abe4c30f8340\n",
      "java.nio.file.NoSuchFileException: /tmp/spark-7a454bd9-abe0-47ee-b10d-4a1cae37ba16/userFiles-087dcaf7-eef5-4b9e-8072-abe4c30f8340\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)\n",
      "\tat sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\n",
      "\tat java.nio.file.Files.readAttributes(Files.java:1737)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "session.sql(SQLQuery11).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346590a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
